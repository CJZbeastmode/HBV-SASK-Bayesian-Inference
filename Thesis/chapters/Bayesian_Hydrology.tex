\chapter{Bayesian Inference and its Use Case in Hydrological Model}

In this chapter, the idea of Bayesian inference will be introduced, including the instantiation of this problem using the Markov chain Monte Carlo algorithm. Afterward, the hydrological model that is used in this paper will be discussed.

\section{Bayesian Inference}
The Bayesian inference problem is a method of statistical inference that is used to calculate the probability estimate based on evidence and the likelihood of the set of parameters.\cite{bayesian_inference} Given a prior distribution that provides information on the preexisting data, the Bayesian inference problem uses the Bayes theorem to update the prior distribution using a likelihood function and derives the actual possibility.

Given the Bayes theorem:\cite{SatzBayes}
\begin{align}
    \Pr[B|A] = \frac{\Pr[B]\cdot\Pr[A|B]}{\Pr[A]}
\end{align}
where $A$ and $B$ are two different incidents. This equation can than be formed into
\begin{align}
    \Pr[B|A] = \frac{\Pr[B]\cdot\Pr[A|B]}{\int\Pr[B]\cdot\Pr[A|B]dB}
\end{align}
$\Pr[A|B]$ is called the likelihood function, which is generated by a set of data to interpret how likely a particular set of observations is.\cite{likelihood_idea} The $\Pr[B]$ is called prior, since this is the preexisting knowledge that is given. The denominator of the equation is called evidence, which is a constant that depicts the probability of observing the data across all values of the model parameters. The result of the above equation is the posterior, which is the object of the Bayesian inference problem.

The different implementations of the Bayesian inference problem are versatile and vary from one another. In this paper, we implement the Bayesian inference problem using the Metropolis-Hastings algorithm with a normal distribution transition kernel. The idea is that we calculate the acceptance rate based on the posterior calculation. For revision, the acceptance rate of the Metropolis-Hastings algorithm with a normal distribution transition kernel is given by:
\begin{align}
    \alpha(X, Y) = \min (\frac{\pi(Y)}{\pi(X)}, 1)
\end{align}
Replacing $\pi(\cdot)$ with the posterior distribution, we derive:
\begin{align}
    \alpha(X, Y) = \min (\frac{\Pr[Y]\Pr[X|Y]}{\Pr[X]\Pr[Y|X]}, 1)
\end{align}
In plain language: The acceptance probability is calculated by the ratio of the prior and likelihood of the newly proposed point over the prior and likelihood of the last generated point. Since the evidence is a constant, they cancel each other out and will therefore not be taken into account.

Different variants of implementations are used throughout this paper to perform Bayesian inference of the hydrological model. They will be discussed in later chapters. For now, we will take a look at the hydrological model.

\section{Overview of the Hydrological Model}
The HBV-SASK conceptual model is a renowned mathematical model that is commonly used in the field of hydrology. HBV is a model that describes the subroutines for snow accumulation and melts, for soil moisture accounting and river routing.\cite{hbv} SASK stands for the province of Saskatchewan, the province in Canada in which the model is developed. The creation of the HBV-SASK model is therefore based on the HBV model but involves local data calibration and integration with local water management needs.\cite{sask}

The HBV-SASK model has twelve different hyperparameters, of which seven are relevant to this thesis. All of these These include:\cite{hydrology}

\begin{itemize}
  \item TT: ranges from -4 to 4, stands for the air temperature threshold in °C for melting/freezing and separating rain and snow
  \item C0: ranges from 0 to 10, describes the base melt factor in mm/°C per day
  \item $\beta$ (beta): ranges from 0 to 3, depicts the shape parameter (exponent) for the soil release equation
  \item ETF: ranges from 0 to 1, describes the temperature anomaly correction in 1/°C of potential evapotranspiration
  \item FC: ranges from 50 to 500, depicts the field capacity of soil in mm.
  \item FRAC: ranges from 0.1 to 0.9, stands for the fraction of soil release entering the fast reservoir
  \item K2: the slow reservoir coefﬁcient ranges from 0 to 0.05, which determines what proportion of the storage is released per day
\end{itemize}

To run this model, these hyperparameters need to be determined. Since the only prior information given is the lowest and the highest bound of each hyperparameter, uncertainty quantification of these hyperparameters is, therefore, necessary to gain posterior information. Apart from these hyperparameters, the starting and the end date of the period that is used for uncertainty quantification are also required to be specified. However, the very first phase at the start is used for the spin-up phase, in which the model runs for some time using historical data. This phase stabilizes internal model states such as soil moisture and groundwater levels, which are important for accurate simulation.
TODO: states

\section{Overview of the Data Set}
There are two existing data sets to the hydrological model, which are respectively called Oldman Basin and Banff Basin, since they are each measured at the Oldman River and in the town of Banff in Alberta, Canada.\cite{hydrology_dataset} The value that is measured is called streamflow. It describes the movement of water within a river or stream channel and is the combined result of all climatological and geographical factors that operate in a drainage basin.\cite{streamflow} Both of these data sets are presented in the format of a time series, in which the value of each measurement is collected against the dates over a long period. The Oldman basin data set is available from 1979 to 2008, whereas the Banff basin data set is available from 1950 to 2011.\cite{hydrology}

Since the data is presented in a format of time series, a time series decomposition is required to present more information on trends, and seasonal and regression effects.\cite{time_series} After the decomposition, the trend, seasonal, and residue of the dataset over the whole period can be observed. For the time series decomposition in this section, the function TSA.seasonal.seasonal\_decompose from the python framework stats models is used.\cite{stat_models_decompose}

First, we take a look at the result of the decomposition of the Oldman basin data set.
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/dataset_time_series/Oldman.png}
\end{center}
The seasonal component of this time series is regular over the years. However, there were significant peaks around the early 1980s, mid-1990s, and around 2005. The streamflow in these periods is higher than anytime else, which indicates that there might be possible heavy rainfalls, floods\cite{hydrology}, or even ecologic disasters. Anomalies in this period can be also found in the residue component, which confirms that there might be odd behaviors happening in these periods. Therefore, quantifying the uncertainty of these three periods would be a challenging task.

We then take a look at the result of the decomposition of the Banff basin data set.
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/dataset_time_series/Banff.png}
\end{center}
Similar to the case of Oldman Basin, it shows a regular seasonal component. The trend, on the other hand, fluctuates across the entire measured period but does not show a general upward or downward direction. This means, that the whole measurement is indeed relatively stable. However, the fluctuation might still impose an influence on the accuracy and stability of uncertainty quantification. The residuals of the Banff basin data set are much more varied than the Oldman basin data set. There are obvious clusters of high activity and anomalies, which confirms that the uncertainty quantification of this data set might be a hard task to deal with.


\section{Task}
Since the task of this paper is to quantify the uncertainty of the input parameters of the hydrology model, the process is therefore as follows: First, the model is created with the configuration of the period and target distributions being given. Then, we select an appropriate Markov chain Monte Carlo algorithm and perform sampling to solve the Bayesian inference. In this process, the output of each generated set of parameter arrays will be calculated by the model. The output will be later compared to the measured data points, which will provide the algorithm with an acceptance rate. In each iteration, the algorithm decides whether to accept or reject this sample. Last but not least, we collect all the samples and perform analysis.

To test the generalized application of the algorithm, we select three challenging yet representative periods for testing, with one being long (5 years), another being medium long (around 2 years) and the other being short (1 year). Due to the constant fluctuation of the Banff basin data set, we could select a long period within it to perform uncertainty quantification. Since the residue in the early 1970s seems to show extreme anomalies, we select 1970 to 1975. The other two periods are both picked from the Oldman Basin data set. The period from 1994 to 1996 is chosen, since there are apparent anomalies of the residue in this period. Another period is the year 2005, since there are also anomalies being showcased in this year.
